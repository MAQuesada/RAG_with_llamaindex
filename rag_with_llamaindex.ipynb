{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG with LlamaIndex and DeciLM**\n",
    "https://deci.ai/blog/rag-with-llamaindex-and-decilm-a-step-by-step-tutorial/\n",
    "\n",
    "Following the five next steps\n",
    "1. Load documents\n",
    "2. Parse Documents into Nodes\n",
    "3. Build an Index\n",
    "4. Query the index\n",
    "5. Parse the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install openai llama_hub llama_index pypdf accelerate sentence_transformers -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.26'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "wget -O state_of_ai_2023.zip https://github.com/harpreetsahota204/langchain-zoomcamp/raw/main/State%20of%20AI%20Report%202023%20-%20ONLINE.pdf.zip\n",
    "unzip state_of_ai_2023.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "wget -O ggml-gpt4all-j-v1.3-groovy.bin https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.llms import OpenAI\n",
    "import json\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.file.pdf.base import PDFReader\n",
    "\n",
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path(\"State of AI Report 2023 - ONLINE.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " docs is a <class 'list'>, of length 163, where each element is a <class 'llama_index.schema.Document'> object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'About the authors  Introduction  | Research  | Industry  | Politics  | Safety  | Predictions #stateofai | 2 \\nstateof.ai 2023 Nathan is the General Partner of Air Street Capital , a \\nventure capital ﬁrm investing in AI-ﬁrst technology \\nand life science companies. He founded RAAIS and \\nLondon.AI (AI community for industry and research), \\nthe RAAIS Foundation (funding open-source AI \\nprojects), and Spinout.fyi (improving university spinout \\ncreation). He studied biology at Williams College and \\nearned a PhD from Cambridge in cancer research. \\nNathan Benaich '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\" docs is a {type(docs0)}, of length {len(docs0)}, where each element is a {type(docs0[0])} object\")\n",
    "docs0[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_slide_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the provided slide text by removing specific patterns and extra whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The raw text from a slide.\n",
    "\n",
    "    Returns:\n",
    "    - str: The cleaned text.\n",
    "\n",
    "    Example:\n",
    "    >>> clean_slide_text(\"LINGO-1 is Wayve’s vision-language-action model ... stateof.ai 2023 #stateofai | 43 \\nLeveraging LLMs for autonomous driving'\")\n",
    "    'LINGO-1 is Wayve’s vision-language-action model ... Leveraging LLMs for autonomous driving'\n",
    "    \"\"\"\n",
    "    # Remove the footer text\n",
    "    text = text.replace(\"stateof.ai 2023\", \"\")\n",
    "\n",
    "    # Remove the header text\n",
    "    text = text.replace(\"Introduction  | Research  | Industry  | Politics  | Safety  | Predictions\", \"\")\n",
    "\n",
    "    # Remove the pattern \"#stateofai | n\"\n",
    "    text = re.sub(r\"#stateofai(\\s*\\|\\s*\\d+)?\", \"\", text)\n",
    "\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "    # Remove any leading or trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def assign_section(document):\n",
    "    \"\"\"\n",
    "    Assigns a section to the document based on its page number.\n",
    "\n",
    "    The function updates the 'metadata' attribute of the document with a key 'section'\n",
    "    that has a value corresponding to the section the page number falls into.\n",
    "\n",
    "    Sections:\n",
    "    - Page 1 through 10: Introduction\n",
    "    - Page 11 through 68: Research\n",
    "    - Page 69 through 120: Politics\n",
    "    - Page 121 through 137: Safety\n",
    "    - Pages 138 and beyond: Predictions\n",
    "\n",
    "    Args:\n",
    "    - document (Document): The Document object to be updated.\n",
    "\n",
    "    Returns:\n",
    "    None. The function updates the Document object in-place.\n",
    "    \"\"\"\n",
    "\n",
    "    page_number = int(document.metadata['page_label'])\n",
    "\n",
    "    if 1 <= page_number <= 10:\n",
    "        document.metadata['section'] = 'Introduction'\n",
    "    elif 11 <= page_number <= 68:\n",
    "        document.metadata['section'] = 'Research'\n",
    "    elif 69 <= page_number <= 120:\n",
    "        document.metadata['section'] = 'Politics'\n",
    "    elif 121 <= page_number <= 137:\n",
    "        document.metadata['section'] = 'Safety'\n",
    "    else:\n",
    "        document.metadata['section'] = 'Predictions'\n",
    "\n",
    "# Iterate through each Document object in docs0\n",
    "for doc in docs0:\n",
    "    # Update the metadata using assign_section\n",
    "    assign_section(doc)\n",
    "\n",
    "    # Metadata keys that are excluded from text for the embed model.\n",
    "    doc.excluded_embed_metadata_keys=['file_name']\n",
    "\n",
    "    # Apply clean_slide_text to the text attribute\n",
    "    doc.text = clean_slide_text(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About the authors \\n Nathan is the General Partner of Air Street Capital , a \\nventure capital ﬁrm investing in AI-ﬁrst technology \\nand life science companies. He founded RAAIS and \\nLondon.AI (AI community for industry and research), \\nthe RAAIS Foundation (funding open-source AI \\nprojects), and Spinout.fyi (improving university spinout \\ncreation). He studied biology at Williams College and \\nearned a PhD from Cambridge in cancer research. \\nNathan Benaich'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs0[1].text # docs0[1].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the **State of AI Report 2023** is a 163 page PDF, it makes sense to first convert the Document objects to Node(chunk) objects.\n",
    "\n",
    "* A smaller chunk_size (e.g., 128) provides more granular chunks. However, there’s a risk that essential information might not be among the top retrieved chunks.\n",
    "\n",
    "* A larger chunk size (e.g., 512) is likely to encompass all necessary information within the top chunks.\n",
    "* As chunk_size increases, more information is directed into the LLM to generate an answer. This can ensure a comprehensive context but might slow down the system.\n",
    "\n",
    "Looking at the State of AI 2023 Report, you’ll see that ideas/concepts/points as grouped as bullet points, mostly seperated by n, \\n●, or \\n-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count for a slide: 127.04907975460122\n",
      "Average word count per bullet point: 10.796663190823775\n",
      "Longest bullet point: 33\n",
      "Average word count in a section: 4141.80\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for bullet points and newlines\n",
    "split_pattern = r\"\\n●|\\n-|\\n\"\n",
    "\n",
    "# Initialize lists to store the word counts of all chunks and entire texts across all documents\n",
    "chunk_word_counts = []\n",
    "entire_text_word_counts = []\n",
    "\n",
    "# Initialize a dictionary to store word counts and slide counts by section\n",
    "section_data = {}\n",
    "\n",
    "# Iterate through each Document object in your list of documents\n",
    "for doc in docs0:\n",
    "    # Split the document's text into chunks based on the pattern\n",
    "    chunks = re.split(split_pattern, doc.text)\n",
    "\n",
    "    # Calculate the number of words in each chunk and store it \n",
    "    chunk_word_counts.extend([len(chunk.split()) for chunk in chunks])\n",
    "\n",
    "    # Calculate the number of words in the entire text and store it\n",
    "    entire_word_count = len(doc.text.split())\n",
    "    entire_text_word_counts.append(entire_word_count)\n",
    "\n",
    "    # Update the word count and slide count for the section in the dictionary\n",
    "    section = doc.metadata['section']\n",
    "    if section in section_data:\n",
    "        section_data[section]['word_count'] += entire_word_count\n",
    "        section_data[section]['slide_count'] += 1\n",
    "    else:\n",
    "        section_data[section] = {'word_count': entire_word_count, 'slide_count': 1}\n",
    "\n",
    "# Calculate the total word count across all sections\n",
    "total_word_count = sum(data['word_count'] for data in section_data.values())\n",
    "\n",
    "# Calculate the number of sections\n",
    "num_sections = len(section_data)\n",
    "\n",
    "# Calculate the average word count across all sections\n",
    "average_word_count_across_sections = total_word_count / num_sections\n",
    "\n",
    "# Calculate summary statistics for chunks\n",
    "average_chunk_word_count = sum(chunk_word_counts) / len(chunk_word_counts)\n",
    "max_chunk_word_count = max(chunk_word_counts)\n",
    "\n",
    "# Calculate average word count for entire texts\n",
    "average_entire_text_word_count = sum(entire_text_word_counts) / len(entire_text_word_counts)\n",
    "\n",
    "print(f\"Average word count for a slide: {average_entire_text_word_count}\")\n",
    "print(f\"Average word count per bullet point: {average_chunk_word_count}\")\n",
    "print(f\"Longest bullet point: {max_chunk_word_count}\")\n",
    "print(f\"Average word count in a section: {average_word_count_across_sections:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "bullet_splitter = SentenceSplitter(paragraph_separator=r\"\\n●|\\n-|\\n\", chunk_size=250)\n",
    "\n",
    "\n",
    "# slides_parser = SimpleNodeParser(\n",
    "#     text_splitter=bullet_splitter,\n",
    "#     include_prev_next_rel=True,\n",
    "#     include_metadata=True\n",
    "#     )\n",
    "# slides_nodes = slides_parser.get_nodes_from_documents(docs0)\n",
    "# NOTE: SimpleNodeParser is deprecated. Use the following instead:\n",
    "\n",
    "slides_nodes = bullet_splitter.get_nodes_from_documents(docs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politics \\n-The world has divided into clear regulatory camps, but progress on global governance remains slower. The largest AI labs are stepping in to ﬁll the vacuum. \\n-The chip wars continue unabated, with the US mobilising its allies, and the Chinese response remaining patchy. \\n-AI is forecast to affect a series of sensitive areas, including elections and employment, but we’re yet to see a signiﬁcant effect. \\nSafety \\n-The existential risk debate has reached the mainstream for the ﬁrst time and intensiﬁed signiﬁcantly. \\n-Many high-performing models are easy to ‘jailbreak’. To remedy RLHF challenges, researchers are exploring alternatives, e.g. self-alignment and pretraining \\nwith human preferences. \\n-As capabilities advance, it’s becoming increasingly hard to evaluate SOTA models consistently. Vibes won’t sufﬁce. Executive Summary'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slides_nodes[26].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SentenceWindowNodeParser**\n",
    "* Node: Represents a unit of text, in this case, a sentence.\n",
    "\n",
    "* Window: A range of sentences surrounding a particular sentence. For example, if the window size is 3, and the current sentence is the 5th sentence, the window will capture sentences 2 to 8.\n",
    "* Metadata: Additional information associated with a node, such as the window of surrounding sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def custom_sentence_splitter(text: str) -> List[str]:\n",
    "    return re.split(r'\\n●|\\n-|\\n', text)\n",
    "\n",
    "bullet_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    sentence_splitter=custom_sentence_splitter,\n",
    "    window_size=3,\n",
    "    include_prev_next_rel=True,\n",
    "    include_metadata=True\n",
    "    )\n",
    "\n",
    "# slides_windows_nodes = bullet_node_parser.get_nodes_from_documents(docs0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IndexNode** is a node object used in LlamaIndex. It represents chunks of the original documents that are stored in an Index. The Index is a data structure that allows for quick retrieval of relevant context for a user query, which is fundamental for RAG use cases.\n",
    "\n",
    "At its core, the IndexNode inherits properties from a TextNode, meaning it primarily represents textual content but the distinguishing feature of an IndexNode is its index_id attribute. This index_id acts as a unique identifier or reference to another object, allowing the node to point or link to other entities within the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_node_parsers = [bullet_node_parser]\n",
    "\n",
    "all_nodes = []\n",
    "\n",
    "for base_node in slides_nodes:\n",
    "    # for each  base_node in slides_nodes, get subnodes with SentenceWindowNodeParser \n",
    "    for parser in sub_node_parsers:\n",
    "        sub_nodes = parser.get_nodes_from_documents([base_node])\n",
    "        \n",
    "        # for each sub_node, create a new IndexNode with the same node_id as the base_node\n",
    "        sub_inodes = [IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM and Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "#  BGE embedder from HuggingFace\n",
    "embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new prompt template\n",
    "template = \"\"\"Below is context that has been retrieved. Your task is to synthesize \\\n",
    "\n",
    "the query, which is delimited by triple backticks,  and write a response that appropriately answers the query based on the retrieved context.\n",
    "\n",
    "### Query:\n",
    "```{query_str}```\n",
    "\n",
    "### Response:\n",
    "\n",
    "Begin!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f62894a4a73406fb5e6edf0e633d79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    # model_name=\"Deci/DeciLM-6b-instruct\",\n",
    "    # tokenizer_name=\"Deci/DeciLM-6b-instruct\",\n",
    "\n",
    "    model_name=\"WeOpenML/Alpaca-7B-v1\",  # alapca of stanford\n",
    "    tokenizer_name=\"WeOpenML/Alpaca-7B-v1\",\n",
    "    query_wrapper_prompt=PromptTemplate(\n",
    "        \"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "    # query_wrapper_prompt=PromptTemplate(template),\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    model_kwargs={'trust_remote_code': True},\n",
    "    generate_kwargs={\"temperature\": 0.0},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "# ServiceContext in LlamaIndex is a utility container that bundles commonly\n",
    "# used resources during the indexing and querying stages of a LlamaIndex pipeline or application\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStoreIndex in LlamaIndex\n",
    "\n",
    "A **VectorStoreIndex** in LlamaIndex is a type of index that uses vector representations of text for efficient retrieval of relevant context. \n",
    "\n",
    "It takes in **IndexNode** objects, which represent chunks of the original documents and uses an embedding model (specified in the **ServiceContext**) to convert the text content of these nodes into vector representations. These vectors are then stored in the VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[1;32m      3\u001b[0m vector_index_chunk \u001b[38;5;241m=\u001b[39m VectorStoreIndex(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mall_nodes\u001b[49m, service_context\u001b[38;5;241m=\u001b[39mservice_context)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index_chunk = VectorStoreIndex(\n",
    "    all_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A retriever is responsible for fetching relevant context from the index given a user query\n",
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecurseiveRetriever in LlamaIndex\n",
    "The **RecursiveRetriever** is designed to recursively explore links from nodes to other retrievers or query engines.\n",
    "\n",
    "This means that when the retriever fetches nodes, if any of those nodes point to another retriever or query engine, the RecursiveRetriever will follow that link and query the linked retriever or engine as well.\n",
    "\n",
    "RecursiveRetriever is designed to handle complex retrieval tasks, especially when data is spread across different retrievers or query engines. It follows links, retrieves data from linked sources, and can combine results from multiple sources into a single coherent response.\n",
    "\n",
    "Here’s a brief explanation of the arguments:\n",
    "\n",
    "* root_id: The root ID of the query graph, in this case you pass \"vector\"\n",
    "* retriever_dict: A dictionary mapping IDs to retrievers.\n",
    "* node_dict: A dictionary that seems to map IDs to nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever_chunk.retrieve(\n",
    "    \"What is FlashAttention?\"\n",
    ")\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RetrieverQueryEngine**\n",
    "\n",
    "A RetrieverQueryEngine in LlamaIndex is a type of query engine that uses a retriever to fetch relevant context from an index given a user query. It is designed to work with retrievers, such as the VectorStoreRetriever created from a VectorStoreIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
    "    retriever_chunk,\n",
    "    service_context=service_context,\n",
    "    verbose=True,\n",
    "    response_mode=\"compact\"\n",
    "    # Compact combines text chunks into larger consolidated chunks that more fully utilize the available context window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you can query the State of AI 2023 Report!\n",
    "response = query_engine_chunk.query(\n",
    "   \"Who are the authors of this report?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "   \"What is new about FlashAttention?\"\n",
    ")\n",
    "str(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
